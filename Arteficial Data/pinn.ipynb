{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1561f48e",
   "metadata": {},
   "source": [
    "Network for arteficial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c378260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b1b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/workspaces/bio-pinn/Arteficial Data/Data_noisy.csv')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y0 = np.array([1., 0., 0.2, 0.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f04cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.actf = torch.tanh\n",
    "        self.f1 = torch.nn.Linear(1, 100)\n",
    "        self.f2 = torch.nn.Linear(100, 100)\n",
    "        self.f3 = torch.nn.Linear(100, 100)\n",
    "        self.f4 = torch.nn.Linear(100, 4)\n",
    "        \n",
    "        self.k1 = torch.tensor(k[0], requires_grad=True).float().to(device)\n",
    "        self.k2 = torch.tensor(k[1], requires_grad=True).float().to(device)\n",
    "        self.k3 = torch.tensor(k[2], requires_grad=True).float().to(device)\n",
    "        self.k4 = torch.tensor(k[3], requires_grad=True).float().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f1(x)\n",
    "        x = self.f2(x)\n",
    "        x = self.f3(x)\n",
    "        x = self.f4(self.actf(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, in_tensor, out_tensor):\n",
    "        self.inp = in_tensor\n",
    "        self.out = out_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx], self.out[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d01f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def phys_loss(inp, out):\n",
    "    dudt = grad(out, inp, grad_outputs=torch.ones_like(out), create_graph=True, allow_unused=True)[0][:,1]\n",
    "    dudx = grad(out, inp, grad_outputs=torch.ones_like(out), create_graph=True, allow_unused=True)[0][:,0]\n",
    "    d2udx2 = grad(dudx, inp, grad_outputs=torch.ones_like(dudx), create_graph=True, allow_unused=True)[0][:,0]\n",
    "    return torch.nn.MSELoss()(d2udx2, 0.5*dudt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3bf88d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m loss_fcn = torch.nn.MSELoss()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, epochs):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_in, batch_out \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_dataloader\u001b[49m:\n\u001b[32m     11\u001b[39m         batch_in = Variable(batch_in, requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m         batch_in.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "k = [1., 1., 1., 1.]\n",
    "from torch.optim import Adam\n",
    "model_pinn = Net(k).to(device)\n",
    "\n",
    "epochs = 1000\n",
    "optimizer_pinn = Adam(model_pinn.parameters(), lr=0.1)\n",
    "loss_fcn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    for batch_in, batch_out in train_dataloader:\n",
    "        batch_in = Variable(batch_in, requires_grad=True)\n",
    "        batch_in.to(device)\n",
    "        batch_out.to(device)\n",
    "        model_pinn.train()\n",
    "        def closure():\n",
    "            optimizer_pinn.zero_grad()\n",
    "            loss = loss_fcn(model_pinn(batch_in), batch_out)\n",
    "            loss += phys_loss(batch_in, model_pinn(batch_in))\n",
    "            loss += loss_fcn(model_pinn(train_in_bd), train_out_bd)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer_pinn.step(closure)\n",
    "    model_pinn.eval()\n",
    "    base_loss = loss_fcn(model_pinn(train_in), train_out)\n",
    "    phys_loss = phys_loss(train_in, model_pinn(train_in))\n",
    "    bdry_loss = loss_fcn(model_pinn(train_in_bd), train_out_bd)\n",
    "    epoch_loss = base + phys + bdry\n",
    "\n",
    "    print(f'Epoch: {epoch+1} | Loss: {round(float(epoch_loss), 4)} = {round(float(base_loss), 4)} + {round(float(phys_loss), 4)} + {round(float(bdry_loss), 4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
